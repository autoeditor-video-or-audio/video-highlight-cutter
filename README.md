# Video Highlight Cutter

Um webapp simples e pr√°tico para **upload de v√≠deos (.mp4)**, extra√ß√£o autom√°tica de **highlights** (momentos importantes) e **download/visualiza√ß√£o** dos cortes diretamente pelo navegador.  
Agora com **op√ß√£o de sele√ß√£o de prompt** para personalizar a detec√ß√£o de highlights e **layout otimizado** para exibir at√© 4 v√≠deos por linha.

---

## üöÄ Como funciona

1. **Upload:** Envie um v√≠deo `.mp4` pela interface web.
2. **Escolha do prompt:** Selecione um prompt pr√©-configurado ou ative a op√ß√£o de **prompt customizado** para usar seu pr√≥prio texto.
3. **Processamento:** O sistema extrai o √°udio, faz transcri√ß√£o autom√°tica (Whisper), detecta os highlights via IA (Ollama) e corta os trechos relevantes.
4. **Acompanhamento em tempo real:** Os highlights aparecem organizados na p√°gina, com at√© 4 v√≠deos por linha, prontos para assistir, baixar ou compartilhar enquanto o processamento ocorre.

---

## üíª Exemplo da interface

Veja como √© f√°cil acompanhar o processo:

![Exemplo da interface Video Highlight Cutter](assets/exemplo-frontend.png)

---

## üõ†Ô∏è Principais Tecnologias

- **Python FastAPI** (backend web)
- **MoviePy** & **Whisper** (processamento e transcri√ß√£o)
- **Ollama** (IA para sele√ß√£o dos melhores highlights)
- **Jinja2** (frontend din√¢mico)
- **TailwindCSS** (layout responsivo)
- **JavaScript** (atualiza√ß√£o din√¢mica de status e sele√ß√£o de prompt)

---

## üê≥ Como subir rapidamente (Recomendado - Docker)

A forma mais f√°cil e recomendada para rodar todo o sistema √© via **Docker Compose**, pois j√° integra todos os servi√ßos necess√°rios (webapp, Whisper, Ollama).  
Siga os passos:

```bash
# 1. Crie a rede docker (apenas na primeira vez)
docker network create app-network

# 2. Suba todos os servi√ßos (webapp, Whisper e Ollama)
docker compose up --build -d

# 3. Acesse normalmente em
http://localhost:8000
```

> **Obs:** O processamento usa IA e pode demorar alguns minutos conforme o v√≠deo e seu hardware.

---

## üë®‚Äçüíª Rodando manualmente (Desenvolvedores/Avan√ßado)

> ‚ö†Ô∏è **Aten√ß√£o:** Para uso real, sempre prefira a vers√£o Docker acima!  
> Rodar localmente sem containers requer que voc√™ tenha o Whisper e o Ollama configurados separadamente.

Para desenvolvedores que desejam rodar *apenas o webapp*:

```bash
# Instale as depend√™ncias Python
pip install -r requirements.txt

# Inicie o servidor web
uvicorn webapp:app --reload --host 0.0.0.0 --port 8000

# Abra o navegador em
http://localhost:8000
```

> Isso sobe apenas a interface web.  
> Para processamento real, os servi√ßos de IA precisam estar rodando conforme no docker-compose!

---

## üìÇ Estrutura do Projeto

- Os arquivos processados ficam na pasta `processed/`
- Highlights podem ser baixados em `.mp4`
- Transcri√ß√µes e cortes intermedi√°rios tamb√©m s√£o salvos
- Interface exibe **at√© 4 v√≠deos por linha** para melhor aproveitamento do espa√ßo

---

## ‚ö° Exemplos de comandos √∫teis

```bash
# Extrai highlights do v√≠deo usando um arquivo de legendas j√° transcrito:
python detect_highlight.py seu_video.srt prompt_detect_highlight.txt
# Gera: seu_video.highlight.json (lista de cortes sugeridos)

# Classifica os cortes usando IA:
python classify_segments.py seu_video.srt seu_video.highlight.json prompt_classify.txt
# Gera: seu_video.highlight.classified.json

# Filtra os melhores highlights:
python filter_highlights.py seu_video.highlight.classified.json seu_video.highlight.filtered.json

# Corta os highlights finais do v√≠deo original:
python cut_highlight.py seu_video.mp4 seu_video.highlight.filtered.json
# Gera arquivos highlight: seu_video_highlight1.mp4, etc.
```

---

## üê≥ docker-compose.yaml (resumido)

```yaml
services:
  video-highlight-cutter:
    build: .
    container_name: video-highlight-cutter
    env_file:
      - .env
    ports:
      - "8000:8000"
    restart: always
    tty: true
    stdin_open: true
    volumes:
      - ./app:/app
    networks:
      - default
    working_dir: /app
    depends_on:
      - whisper
      - ollama

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.9.1
    container_name: whisper
    restart: unless-stopped
    environment:
      - ASR_MODEL=turbo
      - ASR_ENGINE=openai_whisper
    ports:
      - "9000:9000"
    volumes:
      - cache-whisper:/root/.cache
    networks:
      - default
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000"]
      interval: 10s
      timeout: 5s
      retries: 10

  ollama:
    image: ollama/ollama:0.10.1
    container_name: ollama
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_MODELS=llama3.2:3b
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - default

networks:
  default:
    external: true
    name: app-network

volumes:
  cache-whisper:
  ollama:
```

---

## üêç Dockerfile (resumido)

```dockerfile
FROM python:3.11-slim

RUN apt-get -y update && apt-get -y upgrade && apt-get install -y --no-install-recommends libmediainfo0v5 libmediainfo-dev ffmpeg
RUN python -m pip install --upgrade pip
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY ./app .
EXPOSE 8000
CMD ["uvicorn", "webapp:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ‚öôÔ∏è Configura√ß√£o da IA (Ollama)

O comportamento da IA pode ser ajustado pelo arquivo `.env`.  
Esses par√¢metros controlam como o modelo gera as respostas, permitindo mais **precis√£o** ou mais **criatividade**, conforme a necessidade.

```ini
# ------------------------------
# Toggle do motor de IA
# ------------------------------
# false ‚Üí usa Ollama local (padr√£o)
# true  ‚Üí usa API da OpenAI
USE_CHATGPT=false

# ------------------------------
# Configura√ß√£o OpenAI (se USE_CHATGPT=true)
# ------------------------------
OPENAI_API_KEY=sk-xxxxxx
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT=120

# ------------------------------
# Configura√ß√£o Ollama (se USE_CHATGPT=false)
# ------------------------------
# TEMPERATURE ‚Üí controla a aleatoriedade da resposta.
# Baixo (0.1‚Äì0.3) = determin√≠stico, mais preciso.
# Alto (0.7‚Äì1.0) = criativo, mais variado.
OLLAMA_TEMPERATURE=0.1

# TOP_P ‚Üí filtro de probabilidade acumulada (Nucleus Sampling).
# Ex.: 0.8 = considera apenas palavras dentro do top 80% de probabilidade.
# Baixo = mais restrito; alto = mais diverso.
OLLAMA_TOP_P=0.8

# TOP_K ‚Üí limita quantas op√ß√µes de palavras considerar a cada passo.
# Ex.: 50 = avalia apenas as 50 mais prov√°veis.
# Baixo = foco; alto = diversidade.
OLLAMA_TOP_K=50

# REPEAT_PENALTY ‚Üí penaliza repeti√ß√£o de palavras/frases j√° usadas.
# >1 reduz repeti√ß√µes. Ex.: 1.2 √© equil√≠brio comum.
OLLAMA_REPEAT_PENALTY=1.2

# NUM_CTX ‚Üí tamanho do contexto (quantos tokens o modelo "l√™").
# Maior = suporta transcri√ß√µes mais longas, mas exige mais mem√≥ria.
# 8192 tokens ‚âà 6.000 palavras.
OLLAMA_NUM_CTX=16384

# NUM_PREDICT ‚Üí n√∫mero m√°ximo de tokens que o modelo pode gerar.
# Ex.: 256 ‚âà 200 palavras.
# √ötil para limitar sa√≠das longas (ex.: s√≥ JSON).
OLLAMA_NUM_PREDICT=1024
```

> üîß **Dica pr√°tica:**  
> - Para **cortes mais precisos** ‚Üí use TEMPERATURE baixo (0.1‚Äì0.3).  
> - Para **explorar cortes criativos** ‚Üí aumente TEMPERATURE + TOP_P.  
> - Para v√≠deos longos ‚Üí aumente NUM_CTX (se tiver mem√≥ria suficiente).

---

## ‚ÑπÔ∏è Observa√ß√µes

- Os highlights aparecem automaticamente enquanto o processamento ocorre.
- Todos os arquivos processados ficam organizados na pasta `processed/`.
- √â poss√≠vel escolher entre **prompts pr√©-definidos** ou criar um **prompt customizado**.
- O sistema pode baixar o modelo de IA do Ollama automaticamente, se necess√°rio.

---

## ü§ù Contribua!

Achou algum bug, tem uma sugest√£o ou quer contribuir?  
Abra uma issue ou PR!
