API_TRANSCRIBE_URL=whisper #192.168.15.4
API_TRANSCRIBE_PORT=9000
API_TRANSCRIBE_TIMEOUT=4800

OLLAMA_HOSTNAME=ollama #192.168.15.4
OLLAMA_PORT=11434
OLLAMA_MODEL=llama3.2:3b #llama3:8b #gemma3n:e4b-it-fp16 #gemma3n:e4b #gemma2:9b
OLLAMA_TIMEOUT=4800

# OPCIONAL
# ─────────────── OPCIONAIS DE AJUSTE DE INFERÊNCIA ───────────────
# Controlam o "estilo" e a "precisão" das respostas do modelo no Ollama

# TEMPERATURE → controla a aleatoriedade da resposta.
# Valores baixos (0.1–0.3) = respostas mais determinísticas, focadas e consistentes.
# Valores altos (0.7–1.0) = respostas mais criativas, variadas e menos previsíveis.
OLLAMA_TEMPERATURE=0.1

# TOP_P → define a "probabilidade cumulativa" usada no filtro de amostragem (Nucleus Sampling).
# Exemplo: 0.8 significa que o modelo só considera as palavras dentro do top 80% de probabilidade.
# Valores baixos = mais restrito (respostas seguras); valores altos = mais diversidade.
OLLAMA_TOP_P=0.8

# TOP_K → limita o número de palavras candidatas consideradas em cada passo.
# Exemplo: 50 = modelo só avalia as 50 opções mais prováveis antes de escolher.
# Valores baixos = foco, mas risco de respostas “presas”; valores altos = mais diversidade.
OLLAMA_TOP_K=50

# REPEAT_PENALTY → penaliza repetição de palavras/frases já usadas.
# Valores > 1 reduzem repetições (bom para evitar loops ou eco).
# Ex.: 1.2 é um ajuste comum para equilibrar fluidez e evitar redundância.
OLLAMA_REPEAT_PENALTY=1.2

# NUM_CTX → tamanho do contexto (quantidade máxima de tokens que o modelo "lê" de entrada).
# Maior = pode analisar transcrições mais longas ou prompts complexos, mas consome mais memória/VRAM.
# Exemplo: 8192 tokens ≈ 6.000 palavras de janela de contexto.
OLLAMA_NUM_CTX=8192

# NUM_PREDICT → número máximo de tokens que o modelo pode gerar na saída.
# Exemplo: 256 tokens ≈ 200–220 palavras.
# Útil para limitar respostas longas e manter a saída dentro do formato esperado (ex.: só JSON).
OLLAMA_NUM_PREDICT=256

# Classificação de cortes
MIN_SCORE=5